{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKStIP7SZmdx"
      },
      "source": [
        "# Abstractive Text Summarization Using the Transformer Model\n",
        "\n",
        "Abstractive Text Summarization is the task of generating a short and concise summary that captures the salient ideas of the source text. The generated summaries potentially contain new phrases and sentences that may NOT appear in the source text.\n",
        "\n",
        "A transformer is a deep learning model introduced in 2017 by a team at Google brain. It adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. Transformers are increasingly the model of choice for NLP problems, replacing recurrent neural network (RNN) models such as long short-term memory (LSTM).\n",
        "\n",
        "Here I used the transformer model for the task of abstractive text summarization. I used a dataset consisting of 0.5 million reviews of fine foods from amazon (available at https://www.kaggle.com/snap/amazon-fine-food-reviews). Each review comes with a summary of a few words. \n",
        "\n",
        "The model was written using the tensorflow package. The codes were run on google colab where free gpus are available for faster training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U0VLa-jZmd3"
      },
      "source": [
        "##  Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FptMQKUxMLzB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Qz6t9UZmd6"
      },
      "source": [
        "## Check GPU availability\n",
        "In Google colab, click Runtime --> Change runtime type and select GPU as Hardware accelerator. In this project I used a Tesla P100 GPU.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL7NwzG1MLzG",
        "outputId": "3c123a74-fda2-40cf-dde7-831f44c2ebc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa0549iWW7Wk",
        "outputId": "0b7ddcc9-d026-4f13-e768-ff95cda3a050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 14 16:28:30 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    36W / 250W |    375MiB / 16280MiB |      2%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtiqOOq6Zmd9"
      },
      "source": [
        "## Load data\n",
        "First I mounted this colab to my own google drive and change the directory to the path of this project. Then I extracted the \"Text\" and the \"Summary\" column, and excluded rows where either the text or the summary is empty. Finally I shuffled the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-2K1_vdXgL5",
        "outputId": "d4058926-f1a9-4a3c-948f-af6f74cc0324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/text_summerization/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nboetU91aEnV",
        "outputId": "eb074b70-b1a8-42cc-908d-25bc141a735f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/text_summerization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "zen3ERwIMLzI",
        "outputId": "7ca19619-df6d-4af0-f7d2-e1905bb65550"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-88052935-bb62-4991-a25b-93c196318364\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88052935-bb62-4991-a25b-93c196318364')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88052935-bb62-4991-a25b-93c196318364 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88052935-bb62-4991-a25b-93c196318364');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 Summary                                               Text\n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
              "4            Great taffy  Great taffy at a great price.  There was a wid..."
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data = pd.read_csv('Reviews.csv')\n",
        "reviews = data[['Summary','Text']]\n",
        "reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxfoxh_IMLzJ",
        "outputId": "52298b21-68a8-430d-f2a5-a9a1dbb02133"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Summary    27\n",
              "Text        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "reviews.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fHcOi_RGMLzK"
      },
      "outputs": [],
      "source": [
        "reviews = reviews.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "HGDyARLOMLzK",
        "outputId": "6bed0f44-7754-4ea9-9815-0b1ae0ef2e3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8672a5a8-2220-42d5-af38-98dd4fa792ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Like a raspberry chocolate brownie.</td>\n",
              "      <td>This is a very tasty treat. 250 cal., 13g fat ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tastes like coconut water from a fresh coconut...</td>\n",
              "      <td>I've spent at least a hundred bucks on the old...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This tea is the Best!!!!!!!!!!!</td>\n",
              "      <td>I've looked around for many chamomile teas. Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ok flavor, too much like bacon bits</td>\n",
              "      <td>ok flavor,  tasted too much like bacon bits. i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BEST PRICE ANYWHERE!</td>\n",
              "      <td>I hate coffee so I love 5 Hour Energy.  I also...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8672a5a8-2220-42d5-af38-98dd4fa792ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8672a5a8-2220-42d5-af38-98dd4fa792ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8672a5a8-2220-42d5-af38-98dd4fa792ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Summary                                               Text\n",
              "0                Like a raspberry chocolate brownie.  This is a very tasty treat. 250 cal., 13g fat ...\n",
              "1  Tastes like coconut water from a fresh coconut...  I've spent at least a hundred bucks on the old...\n",
              "2                    This tea is the Best!!!!!!!!!!!  I've looked around for many chamomile teas. Th...\n",
              "3                ok flavor, too much like bacon bits  ok flavor,  tasted too much like bacon bits. i...\n",
              "4                               BEST PRICE ANYWHERE!  I hate coffee so I love 5 Hour Energy.  I also..."
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "# shuffling the data \n",
        "reviews = shuffle(reviews)\n",
        "reviews = reviews.reset_index(drop=True)\n",
        "reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SQSXEmOnMLzL"
      },
      "outputs": [],
      "source": [
        "summary, review_text = pd.DataFrame(), pd.DataFrame()\n",
        "summary['Summary'] = reviews['Summary']\n",
        "review_text['Text'] = reviews['Text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl5Ge38IZmeC"
      },
      "source": [
        "## Clean data\n",
        "I (i) made all words lowercase, (ii) removed unwanted characters such as http links, (iii) removed stop words such as \"the\", \"one\", ..., which are commonly used but do not contirbute to the meaning of a sentence, and (iv) converted abbreviations to the long form (for example \"ain't\" to \"is not\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew4jn0tsMLzL",
        "outputId": "f5b6a4b5-8b2e-46f1-d51c-da8a261de06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "short form: ain't ;  long form: is not\n"
          ]
        }
      ],
      "source": [
        "contraction_mapping = np.load('contraction_mapping.npy', allow_pickle=True).item()\n",
        "(short,long) = next(iter(contraction_mapping.items()))\n",
        "print('short form:', short, ';  long form:', long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ws_ouno5MLzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acea89e2-281c-418f-e5ec-b00431da758a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "imV95Kj5MLzN"
      },
      "outputs": [],
      "source": [
        "def clean_data(text, remove_stopwords=False):\n",
        "    \n",
        "    # Make all words lowercase\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    \n",
        "    text = text.split()\n",
        "    \n",
        "    # Remove stopwords that are not important for the meaning of a sentence\n",
        "    if(remove_stopwords):\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in stop_words:\n",
        "                continue\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = new_text\n",
        "    \n",
        "    # Replace contractions with their longer forms \n",
        "    new_text = []\n",
        "    for word in text:\n",
        "        if word in contraction_mapping:\n",
        "            new_text.append(contraction_mapping[word])\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    text = \" \".join(new_text)\n",
        "      \n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    return text\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GDD_uochMLzN"
      },
      "outputs": [],
      "source": [
        "summary['Summary'] = summary['Summary'].apply(lambda x: clean_data(x, remove_stopwords=True))\n",
        "review_text['Text'] = review_text['Text'].apply(lambda x: clean_data(x, remove_stopwords=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ABJbSViOZmeF"
      },
      "outputs": [],
      "source": [
        "# Remove data which only consists of stopwords.\n",
        "ind = (summary['Summary']!='')&(review_text['Text']!='')\n",
        "summary = summary[ind]\n",
        "review_text = review_text[ind]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "QqpAndNRMLzO",
        "outputId": "2396d369-e5a2-4580-d7b2-8a7684e4911d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c5471676-60c5-430d-96f9-20078ab6a2ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>like raspberry chocolate brownie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tastes like coconut water fresh coconut stored...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tea best</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ok flavor much like bacon bits</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>best price anywhere</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5471676-60c5-430d-96f9-20078ab6a2ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5471676-60c5-430d-96f9-20078ab6a2ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5471676-60c5-430d-96f9-20078ab6a2ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Summary\n",
              "0                   like raspberry chocolate brownie\n",
              "1  tastes like coconut water fresh coconut stored...\n",
              "2                                           tea best\n",
              "3                     ok flavor much like bacon bits\n",
              "4                                best price anywhere"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "summary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "9lPCjklnMLzO",
        "outputId": "4ae62caa-2126-4cc8-a8eb-7c64fe1f0f7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9ef061bf-8676-499e-968d-9f1dc7384dcb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tasty treat 250 cal 13g fat 2g saturated chole...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i have spent least hundred bucks old zico boxe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i have looked around many chamomile teas one b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ok flavor tasted much like bacon bits got sale...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hate coffee love 5 hour energy also hate payin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ef061bf-8676-499e-968d-9f1dc7384dcb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ef061bf-8676-499e-968d-9f1dc7384dcb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ef061bf-8676-499e-968d-9f1dc7384dcb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                Text\n",
              "0  tasty treat 250 cal 13g fat 2g saturated chole...\n",
              "1  i have spent least hundred bucks old zico boxe...\n",
              "2  i have looked around many chamomile teas one b...\n",
              "3  ok flavor tasted much like bacon bits got sale...\n",
              "4  hate coffee love 5 hour energy also hate payin..."
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "review_text.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "3Cl4p4vQMLzP",
        "outputId": "0a3856cd-dec0-4567-fba7-cb010fe61150"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ff7b11dc-b84b-41a6-a3e8-4442bf10387c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Like a raspberry chocolate brownie.</td>\n",
              "      <td>This is a very tasty treat. 250 cal., 13g fat ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tastes like coconut water from a fresh coconut...</td>\n",
              "      <td>I've spent at least a hundred bucks on the old...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This tea is the Best!!!!!!!!!!!</td>\n",
              "      <td>I've looked around for many chamomile teas. Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ok flavor, too much like bacon bits</td>\n",
              "      <td>ok flavor,  tasted too much like bacon bits. i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BEST PRICE ANYWHERE!</td>\n",
              "      <td>I hate coffee so I love 5 Hour Energy.  I also...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff7b11dc-b84b-41a6-a3e8-4442bf10387c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff7b11dc-b84b-41a6-a3e8-4442bf10387c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff7b11dc-b84b-41a6-a3e8-4442bf10387c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Summary                                               Text\n",
              "0                Like a raspberry chocolate brownie.  This is a very tasty treat. 250 cal., 13g fat ...\n",
              "1  Tastes like coconut water from a fresh coconut...  I've spent at least a hundred bucks on the old...\n",
              "2                    This tea is the Best!!!!!!!!!!!  I've looked around for many chamomile teas. Th...\n",
              "3                ok flavor, too much like bacon bits  ok flavor,  tasted too much like bacon bits. i...\n",
              "4                               BEST PRICE ANYWHERE!  I hate coffee so I love 5 Hour Energy.  I also..."
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "reviews.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlU7kPSiMLzP"
      },
      "source": [
        "## Length statistics\n",
        "Here I counted the number of words in the cleaned review texts and cleaned summary. The statistics are important to decide how I pad the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "B5tevc19MLzQ"
      },
      "outputs": [],
      "source": [
        "def count_length(data):\n",
        "    length = {}\n",
        "    for text in data:\n",
        "        l = len(text.split())\n",
        "        if(l not in length):\n",
        "            length[l] = 1\n",
        "        else:\n",
        "            length[l] += 1\n",
        "    \n",
        "    stats = np.zeros(max(length)+1)\n",
        "    \n",
        "    for i in length:\n",
        "        stats[i] = length[i]\n",
        "    total = np.sum(stats)\n",
        "    cdf = np.cumsum(stats) / total\n",
        "    \n",
        "    return stats, cdf\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "A5z-5QuhMLzQ"
      },
      "outputs": [],
      "source": [
        "text_length, text_length_cdf = count_length(review_text['Text'])\n",
        "summary_length, summary_length_cdf = count_length(summary['Summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "0JY66Gj5MLzR",
        "outputId": "2fe620a6-40f9-4c0a-b16c-69e262789f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For long review texts, 90% of the data has <= 85 words, 95% of the data has <= 117 words,  and 99% of the data has <= 211 words\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAe8UlEQVR4nO3dfbRVdb3v8fdHzHwWlB0hSBsN7aijMPf14fpwSU3RumFmJZXiwxEdyenBGoV1RnYtx6B7LNPy4iAl8RyDuBpJSRFpIo0bCiRXAfO6VZRNCAg+HTUL/N4/5m/JZLvX3ovNnmutvdbnNcYce87vfPrNxdz7y+83f+s3FRGYmZkVaZdaF8DMzBqfk42ZmRXOycbMzArnZGNmZoVzsjEzs8I52ZiZWeGcbKxfk7Ra0mm1LsfOkLRS0phal8OsSE42ZjUWEUdExP1FHV/ShZL+WG/HsubiZGO2kyTtWusymNU7JxtrGJLeKemHkv6aph9KemdaN0ZSh6SvSNogaZ2ki3L7HiDpV5JelrRE0nfL/Q9eUqukkHSJpGeB+1L8YkmPSXpB0nxJ70nxqZKu63SMuyVdmebfagqUtIukyZKelLRJ0mxJ+6d1MyR9Jc0PS2W4Ii0fImmzpF06neefgJuB4yX9p6QXc5/VdZKelbRe0s2S9kjr5kn6fu4YsyRN7+ZYZ0laJekVSWslfbVX/4DW0JxsrJF8EzgOGA18ADgG+Nfc+ncD+wHDgEuAmyQNSutuAl5N20xIU0/+G/BPwBmSxgHfAM4BWoBFwMy03Uzg05IEkM55OjCri2P+C3B2OvaBwAupbAALgTG5cz8FnJxbXhQRb+YPFhGPAZcDf4qIvSNiYFo1BTiU7LN6b/pMvpXWXQycL+kUSZ8l+xy/2M2xbgUui4h9gCNJyddsOxHhyVO/nYDVwGlp/kngrNy6M4DVaX4M8Dqwa279BrLkNAD4B3BYbt13gT+WOWcrEMDBudhvgEtyy7sArwHvAQQ8C5yc1l0K3FfmGh4DTs2tG5rKtitwCFny2YWshnEZ0JG2mwFcWaa8F+avJZXnVeCQXOx44Onc8ieANcDzwInljpViz6ay7Fvr+8FT/U6u2VgjORB4Jrf8TIqVbIqILbnl14C9yWoiu5L9cS3Jz5eT3+Y9wA2SXkzNS5vJ/qgPi4ggq8WMT9t+BrijzDHfA8zJHecxYCswJCKeJEsSo4GTgF8Df5V0GFnNZmEFZYbsevcEluXO89sUL/kVWRJ+PCJ66hDwCeAs4BlJCyUdX2E5rIk42Vgj+SvZH+uSESnWk43AFmB4LnZQBfvlh0xfQ9aUNDA37RER/yetnwmcm57jHAvcVeaYa4AzOx1n94hYm9YvBM4FdkuxhWRNfoOA5RWUE7LayuvAEblz7BcRe+e2uZYs0Q2VND4Xf9sw8RGxJCLGAe8CfgnMLlMOa2JONtZIZgL/KqlF0mCyZxD/0dNOEbEV+AXwbUl7SnofcMEOnvtm4CpJRwBI2k/SJ3PneJjsj/wtwPyIeLGb41yb61zQkp4HlSwEJgEPpOX70/If03V0ZT0wXNJuqSxvAj8Brpf0rnSeYZLOSPMnAxeRfQYTgB9JGtbVsSTtJumzkvaLiH8ALwPbPTcyAycbayzfBZYCjwCPAn9OsUpMIus88Bzw72SJ641KTxwRc4DvAbMkvQysAM7stNnPgNPSz3JuAOYCv5P0CrCYrCZUshDYh23J5o9kTWIPUN59wErgOUnPp9jXgXZgcSrv74HDJO0L3A5Mioi1EbGIrAPAT1MHh66OdT6wOh3ncuCz3ZTFmpSy5mQzy5P0PeDdEVFJrzQz64FrNmaApPdJer8yx5B1jZ5T63KZNQp/89kssw9Z09mBZM8lvg/cXdMSmTUQN6OZmVnh3IxmZmaFa7pmtMGDB0dra2uti2Fm1q8sW7bs+Yho6XnLrjVdsmltbWXp0qW1LoaZWb8i6ZmetyrPzWhmZlY4JxszMyuck42ZmRXOycbMzApXWLJJb/bbIGlFLvZzScvTtFrS8hRvlfR6bt3NuX2OlvSopHZJN+ZeQLW/pAWSnkg/B729FGZmVg+KrNncBozNByLi0xExOiJGkw2x/ovc6idL6yLi8lx8KtnLpkalqXTMycC9ETEKuDctm5lZHSos2UTEA2QvkHqbVDv5FNtem9slSUPJ3v63OL2A6nayV+YCjCN7OyHp59ldHMLMzOpArZ7ZnASsj4gncrGRkh5Ob/o7KcWGAR25bTpSDLI3F65L888BQ8qdTNJESUslLd24cWMfXYKZmVWqVslmPNvXatYBIyLiKOBK4GfpvRoVSbWesoO8RcS0iGiLiLaWll5/AdbMzHqp6iMISNoVOAc4uhSLiDdIL6qKiGWSngQOBday/at6h6cYwHpJQyNiXWpu21CN8tdC6+R73ppfPeUjNSyJmVnv1GK4mtOAv0TEW81jklqAzRGxVdLBZB0BnoqIzZJelnQc8CDZa2p/lHabS/bK2inpZ0MNB59PMGZm/V2RXZ9nAn8ie9Vsh6RL0qrzeHvHgJOBR1JX6DuByyOi1Lng82TvbW8HngR+k+JTgA9LeoIsgU0p6lrqTevke5yMzKxfKaxmExHjy8Qv7CJ2F1lX6K62Xwoc2UV8E3DqzpXSzMyqwSMImJlZ4ZxszMyscE42ZmZWOCcbMzMrnJONmZkVzsnGzMwK52RjZmaFc7IxM7PCOdmYmVnhnGzMzKxwTjZmZla4Woz6bH3Irx8ws/7ANRszMyucazZ1pFRL6W0NxbUcM6tXrtmYmVnhnGzMzKxwTjZmZlY4JxszMyuck42ZmRXOycbMzArnZGNmZoUrLNlImi5pg6QVudi3Ja2VtDxNZ+XWXSWpXdLjks7IxcemWLukybn4SEkPpvjPJe1W1LWYmdnOKbJmcxswtov49RExOk3zACQdDpwHHJH2+V+SBkgaANwEnAkcDoxP2wJ8Lx3rvcALwCUFXouZme2EwpJNRDwAbK5w83HArIh4IyKeBtqBY9LUHhFPRcTfgVnAOEkCTgHuTPvPAM7u0wtoAK2T79luVAEzs1qpxTObSZIeSc1sg1JsGLAmt01HipWLHwC8GBFbOsW7JGmipKWSlm7cuLGvrsPMzCpU7WQzFTgEGA2sA75fjZNGxLSIaIuItpaWlmqc0szMcqo6EGdErC/NS/oJ8Ou0uBY4KLfp8BSjTHwTMFDSrql2k9/ezMzqTFVrNpKG5hY/DpR6qs0FzpP0TkkjgVHAQ8ASYFTqebYbWSeCuRERwB+Ac9P+E4C7q3ENZma24wqr2UiaCYwBBkvqAK4GxkgaDQSwGrgMICJWSpoNrAK2AFdExNZ0nEnAfGAAMD0iVqZTfB2YJem7wMPArUVdi5mZ7ZzCkk1EjO8iXDYhRMS1wLVdxOcB87qIP0XWW83MzOqcRxAwM7PCOdmYmVnh/FroJuFXRptZLTnZ1JATgJk1CzejmZlZ4ZxszMyscE42ZmZWOCcbMzMrnJONmZkVzsnGzMwK567PTchdrs2s2lyzMTOzwjnZmJlZ4ZxszMyscE42ZmZWOCcbMzMrnJON0Tr5nu16qJmZ9TUnGzMzK5yTjZmZFc7JxszMCldYspE0XdIGSStysX+T9BdJj0iaI2lgirdKel3S8jTdnNvnaEmPSmqXdKMkpfj+khZIeiL9HFTUtZiZ2c4psmZzGzC2U2wBcGREvB/4f8BVuXVPRsToNF2ei08FLgVGpal0zMnAvRExCrg3LdtOKnUWcIcBM+tLhSWbiHgA2Nwp9ruI2JIWFwPDuzuGpKHAvhGxOCICuB04O60eB8xI8zNycTMzqzO1fGZzMfCb3PJISQ9LWijppBQbBnTktulIMYAhEbEuzT8HDCm0tGZm1ms1GfVZ0jeBLcAdKbQOGBERmyQdDfxS0hGVHi8iQlJ0c76JwESAESNG9L7gZmbWK1Wv2Ui6EPgo8NnUNEZEvBERm9L8MuBJ4FBgLds3tQ1PMYD1qZmt1Ny2odw5I2JaRLRFRFtLS0sfX5GZmfWkqslG0ljga8DHIuK1XLxF0oA0fzBZR4CnUjPZy5KOS73QLgDuTrvNBSak+Qm5uJmZ1ZnCmtEkzQTGAIMldQBXk/U+eyewIPVgXpx6np0MXCPpH8CbwOURUepc8Hmynm17kD3jKT3nmQLMlnQJ8AzwqaKupVn5JWtm1lcKSzYRMb6L8K1ltr0LuKvMuqXAkV3ENwGn7kwZzcysOjyCgJmZFc7JxszMCudkY2ZmhXOyMTOzwjnZmJlZ4ZxszMyscDUZrqaZ9efRlP29GzPrLddszMyscE42ZmZWOCcbMzMrnJONmZkVzsnGzMwK52RjZmaFc7IxM7PCOdmYmVnhnGzMzKxwHkHAesWjCZjZjnDNxvpE6+R7+vVQPGZWLCcbMzMrnJONmZkVzsnGzMwKV2iykTRd0gZJK3Kx/SUtkPRE+jkoxSXpRkntkh6R9MHcPhPS9k9ImpCLHy3p0bTPjZJU5PWYmVnvFF2zuQ0Y2yk2Gbg3IkYB96ZlgDOBUWmaCEyFLDkBVwPHAscAV5cSVNrm0tx+nc9lZmZ1oNBkExEPAJs7hccBM9L8DODsXPz2yCwGBkoaCpwBLIiIzRHxArAAGJvW7RsRiyMigNtzxzIzszpS0fdsJN0bEaf2FKvQkIhYl+afA4ak+WHAmtx2HSnWXbyji3hX5Z9IVltixIgRvSiy9Za/j2Nm0EOykbQ7sCcwODVdlZ6J7EuZP+w7IiJCUuzscSo4zzRgGkBbW1vh52t2/r6NmXXWU83mMuBLwIHAMrYlm5eBH/fynOslDY2IdakpbEOKrwUOym03PMXWAmM6xe9P8eFdbG9mZnWm22c2EXFDRIwEvhoRB0fEyDR9ICJ6m2zmAqUeZROAu3PxC1KvtOOAl1Jz23zgdEmDUu3qdGB+WveypONSL7QLcscyM7M6UtEzm4j4kaT/CrTm94mI27vbT9JMslrJYEkdZL3KpgCzJV0CPAN8Km0+DzgLaAdeAy5K59gs6TvAkrTdNRFR6nTwebIeb3sAv0mTmZnVmUo7CPw7cAiwHNiawqUeYGVFxPgyq97WsSD1KLuizHGmA9O7iC8FjuyuDFZfSs9z3FnArLlUOupzG3B4SghmZmY7pNJkswJ4N7Cupw3NKuVu0WbNo9JkMxhYJekh4I1SMCI+VkipzMysoVSabL5dZCHMzKyxVdobbWHRBTEzs8ZVaW+0V8h6nwHsBrwDeDUi9i2qYGZm1jgqrdnsU5pPX6AcBxxXVKHMzKyx7PCoz2lU5l+SjcZsZmbWo0qb0c7JLe5C9r2bvxVSImtK7gZt1tgq7Y3233PzW4DVZE1pZmZmPar0mc1FRRfEzMwaV0XPbCQNlzRH0oY03SVpeM97mpmZVd5B4KdkrwA4ME2/SjEzM7MeVZpsWiLipxGxJU23AS0FlquhtE6+x2+vNLOmVmmy2STpc5IGpOlzwKYiC2ZmZo2j0mRzMdlLzp4jG/n5XODCgspk5tqgWYOptOvzNcCEiHgBQNL+wHVkScjMzKxbldZs3l9KNJC9qhk4qpgimZlZo6m0ZrOLpEGdajaV7mu2U8o1p3mkAbP+o9KE8X3gT5L+d1r+JHBtMUUyM7NGU+kIArdLWgqckkLnRMSq4oplZmaNpOKmsJRcdjrBSDoM+HkudDDwLWAgcCmwMcW/ERHz0j5XAZcAW4EvRMT8FB8L3AAMAG6JiCk7Wz4zM+t7VX/uEhGPA6MBJA0A1gJzgIuA6yPiuvz2kg4HzgOOIBu94PeSDk2rbwI+DHQASyTNdY3LzKz+1Poh/6nAkxHxTPZOti6NA2ZFxBvA05LagWPSuvaIeApA0qy0rZONmVmd2eGXp/Wx84CZueVJkh6RNF3SoBQbBqzJbdORYuXibyNpoqSlkpZu3Lixq03MzKxANUs2knYDPgaUerhNBQ4ha2JbR9YDrk9ExLSIaIuItpYWD+lmZlZttWxGOxP4c0SsByj9BJD0E+DXaXEtcFBuv+EpRjdxaxKl7+D4Ozdm9a2WyWY8uSY0SUMjYl1a/DiwIs3PBX4m6QdkHQRGAQ8BAkZJGkmWZM4DPlOlslsd8qulzepXTZKNpL3IepFdlgv/T0mjgSB77fRlABGxUtJssgf/W4ArImJrOs4kYD5Z1+fpEbGyahdhZmYVq0myiYhXgQM6xc7vZvtr6WLEgvQ9nHl9XkAzM+tTte6NZmZmTaDW37MxK4Sf35jVF9dszMyscE42ZmZWOCcbawp+zbRZbfmZjTUdP88xqz7XbMzMrHBONmZmVjgnGzMzK5yTjZmZFc7JxszMCufeaNbU3DPNrDpcszEzs8I52Zjl+MufZsVwsjEzs8L5mY1ZGX6eY9Z3XLMxM7PCOdmYmVnhnGzMzKxwfmZTALf1N57O/6alZf/7mlXGNRszMytczZKNpNWSHpW0XNLSFNtf0gJJT6Sfg1Jckm6U1C7pEUkfzB1nQtr+CUkTanU9ZmZWXq2b0T4UEc/nlicD90bEFEmT0/LXgTOBUWk6FpgKHCtpf+BqoA0IYJmkuRHxQjUvwpqbm03NelZvzWjjgBlpfgZwdi5+e2QWAwMlDQXOABZExOaUYBYAY6tdaDMz614tk00Av5O0TNLEFBsSEevS/HPAkDQ/DFiT27cjxcrFtyNpoqSlkpZu3LixL6/BzMwqUMtmtBMjYq2kdwELJP0lvzIiQlL0xYkiYhowDaCtra1PjmnWFTepmXWtZskmItamnxskzQGOAdZLGhoR61Iz2Ya0+VrgoNzuw1NsLTCmU/z+gotuVjEnH7NMTZrRJO0laZ/SPHA6sAKYC5R6lE0A7k7zc4ELUq+044CXUnPbfOB0SYNSz7XTU8zMzOpIrWo2Q4A5kkpl+FlE/FbSEmC2pEuAZ4BPpe3nAWcB7cBrwEUAEbFZ0neAJWm7ayJic/Uuw8zMKlGTZBMRTwEf6CK+CTi1i3gAV5Q51nRgel+X0ayvuUnNmlm9dX02axp+UZs1EycbMzMrXK1HEDAz3MRmjc/JxqzOOPFYI3IzmpmZFc7JxszMCudkY1bn3GvNGoGTjZmZFc7JxszMCufeaGb9iHuqWX/lZGPWTznxWH/iZGPWIErJZ/WUjzgRWd3xMxszMyucazZmDc61HKsHrtmYmVnhnGzMzKxwTjZmTcYjElgt+JmNmQF+tmPFcrIxa2Ku4Vi1uBnNzLrk5jbrS67Z9BH/UpqZlVf1mo2kgyT9QdIqSSslfTHFvy1praTlaTort89VktolPS7pjFx8bIq1S5pc7WsxaxalWo7/U2W9VYuazRbgKxHxZ0n7AMskLUjrro+I6/IbSzocOA84AjgQ+L2kQ9Pqm4APAx3AEklzI2JVVa7CrEl17kiQHybHrJyqJ5uIWAesS/OvSHoMGNbNLuOAWRHxBvC0pHbgmLSuPSKeApA0K23rZGNmVmdq+sxGUitwFPAgcAIwSdIFwFKy2s8LZIlocW63DrYlpzWd4seWOc9EYCLAiBEj+u4CzGw77j5t5dSsN5qkvYG7gC9FxMvAVOAQYDRZzef7fXWuiJgWEW0R0dbS0tJXhzWzbnR+zuNnPs2tJjUbSe8gSzR3RMQvACJifW79T4Bfp8W1wEG53YenGN3EzcysjlQ92UgScCvwWET8IBcfmp7nAHwcWJHm5wI/k/QDsg4Co4CHAAGjJI0kSzLnAZ+pzlWY2c4oV8Nx01vjqkXN5gTgfOBRSctT7BvAeEmjgQBWA5cBRMRKSbPJHvxvAa6IiK0AkiYB84EBwPSIWFnNCzGzvufnPo2pFr3R/khWK+lsXjf7XAtc20V8Xnf7mVlj8dtI+y+PIGBmdcsJpXF4bDQzMyucazZm1u/1tgbk0Q+qx8nGzBqOn+3UHycbM2sa3SUed8culpONmdkOcE2pd5xsdoLbe82aQ7nfdSeeyjnZmJkVwIloe042ZmZ9pDetHc2SlJxszMwK5hfOOdmYmdWN7pJSf/8ukZONmVk/VC4R5e1Ih4aik5KHqzEzs8K5ZmNm1oR68wXXneGajZmZFc7JxszMCudkY2ZmhfMzmx3QLF++MjPra67ZmJlZ4ZxszMyscE42ZmZWuH7/zEbSWOAGYABwS0RM2dFjVLu/uZlZs+nXyUbSAOAm4MNAB7BE0tyIWNXTvt0lkXoZS8jMrFH092a0Y4D2iHgqIv4OzALG1bhMZmbWiSKi1mXoNUnnAmMj4p/T8vnAsRExqdN2E4GJafFIYEVVC1q/BgPP17oQdcKfxTb+LLbxZ7HNYRGxT2937tfNaJWKiGnANABJSyOircZFqgv+LLbxZ7GNP4tt/FlsI2npzuzf35vR1gIH5ZaHp5iZmdWR/p5slgCjJI2UtBtwHjC3xmUyM7NO+nUzWkRskTQJmE/W9Xl6RKzsYbdpxZes3/BnsY0/i238WWzjz2Kbnfos+nUHATMz6x/6ezOamZn1A042ZmZWuKZJNpLGSnpcUrukybUuTzVJOkjSHyStkrRS0hdTfH9JCyQ9kX4OqnVZq0XSAEkPS/p1Wh4p6cF0f/w8dThpeJIGSrpT0l8kPSbp+Ga9LyR9Of1+rJA0U9LuzXJfSJouaYOkFblYl/eBMjemz+QRSR+s5BxNkWxyw9qcCRwOjJd0eG1LVVVbgK9ExOHAccAV6fonA/dGxCjg3rTcLL4IPJZb/h5wfUS8F3gBuKQmpaq+G4DfRsT7gA+QfSZNd19IGgZ8AWiLiCPJOhydR/PcF7cBYzvFyt0HZwKj0jQRmFrJCZoi2dDkw9pExLqI+HOaf4XsD8owss9gRtpsBnB2bUpYXZKGAx8BbknLAk4B7kybNMVnIWk/4GTgVoCI+HtEvEiT3hdkvXP3kLQrsCewjia5LyLiAWBzp3C5+2AccHtkFgMDJQ3t6RzNkmyGAWtyyx0p1nQktQJHAQ8CQyJiXVr1HDCkRsWqth8CXwPeTMsHAC9GxJa03Cz3x0hgI/DT1KR4i6S9aML7IiLWAtcBz5IlmZeAZTTnfVFS7j7o1d/TZkk2BkjaG7gL+FJEvJxfF1kf+IbvBy/po8CGiFhW67LUgV2BDwJTI+Io4FU6NZk10X0xiOx/7COBA4G9eHuzUtPqi/ugWZJN0w9rI+kdZInmjoj4RQqvL1V/088NtSpfFZ0AfEzSarLm1FPInlsMTM0n0Dz3RwfQEREPpuU7yZJPM94XpwFPR8TGiPgH8Auye6UZ74uScvdBr/6eNkuyaephbdIziVuBxyLiB7lVc4EJaX4CcHe1y1ZtEXFVRAyPiFay++C+iPgs8Afg3LRZs3wWzwFrJB2WQqcCq2jC+4Ks+ew4SXum35fSZ9F090VOuftgLnBB6pV2HPBSrrmtrKYZQUDSWWRt9aVhba6tcZGqRtKJwCLgUbY9p/gG2XOb2cAI4BngUxHR+SFhw5I0BvhqRHxU0sFkNZ39gYeBz0XEG7UsXzVIGk3WUWI34CngIrL/hDbdfSHpfwCfJuu9+TDwz2TPIhr+vpA0ExhD9kqF9cDVwC/p4j5IyfjHZM2MrwEXRUSPI0I3TbIxM7PaaZZmNDMzqyEnGzMzK5yTjZmZFc7JxszMCudkY2ZmhXOyMdsJku6X1FaF83whjcp8R9HnSue7TdK5PW9pVpl+/Vpos/5M0q65cbd68nngtIjoqHE5zHrFNRtreJJaU63gJ+l9Jb+TtEda91bNRNLgNIwNki6U9Mv0Ho/VkiZJujINWLlY0v65U5wvaXl6D8oxaf+90jtCHkr7jMsdd66k+8iGbe9c1ivTcVZI+lKK3QwcDPxG0pc7bX+PpPen+YclfSvNXyPp0vQt739Lx3tU0qfT+jGSFkmaC6xK2/1Y2Tuffg+8K3eOKcrehfSIpOv64J/EmpBrNtYsRgHjI+JSSbOBTwD/0cM+R5KNkL070A58PSKOknQ9cAHZiBQAe0bEaEknA9PTft8kGwrnYkkDgYfSH3HIxh97f+dv5Us6muwb/McCAh6UtDAiLpc0FvhQRDzfqYyLgJMkPUP2zfcTUvwk4HLgHGA02btqBgNLJD2QK8eREfG0pHOAw8je9zSEbKiW6ZIOAD4OvC8iIl2L2Q5zzcaaxdMRsTzNLwNaK9jnDxHxSkRsJBty/lcp/min/WfCW+8E2Tf9QT4dmCxpOXA/WcIakbZfUGb4lxOBORHxakT8J9lgkCf1UMZFZO+kOQG4B9hb0p7AyIh4PB1zZkRsjYj1wELgv6R9H4qIp9P8ybnt/grcl+IvAX8Dbk0J6bUeymPWJddsrFnkx7PaCuyR5rew7T9du3ezz5u55TfZ/nen85hPQVYz+UT6g/8WSceSDeXfV5YAbWTjmi0gq71cSpZQe9JjOSJiS2oaPJVsQMpJZCNlm+0Q12ys2a0Gjk7zve19VXoOciLZCLgvAfOBf0mDFiLpqAqOswg4O408vBdZ89Wi7nZIb55dA3wS+FPa/qtAqalsEfBpSQMktZDVYB7q4lAP5LYbCnwolXtvYL+ImAd8maw5zmyHuWZjze46YLakiWTNUL3xN0kPA+8ALk6x75A903lE0i7A08BHuztIRPxZ0m1sSwa3RMTDFZx/EXBqRLwuaRHZ+0VKSWoOcDzwf8lqXF+LiOckva/TMeaQ1VhWkQ23/6cU3we4W9LuZLW1Kysoj9nbeNRnMzMrnJvRzMyscE42ZmZWOCcbMzMrnJONmZkVzsnGzMwK52RjZmaFc7IxM7PC/X/IWz8kBPR+kwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.bar(np.arange(len(text_length)),text_length)\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('number of words')\n",
        "plt.xlim([0,100])\n",
        "plt.title('long review texts')\n",
        "print('For long review texts, 90% of the data has <=', np.sum(text_length_cdf<0.9), 'words,', '95% of the data has <=', np.sum(text_length_cdf<0.95), 'words,', ' and 99% of the data has <=', np.sum(text_length_cdf<0.99), 'words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "_add2WAJMLzR",
        "outputId": "fb59eda1-51f1-46cf-ecdc-67025a5b34d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For summaries, 90% of the data has <= 5 words, 95% of the data has <= 6 words,  and 99% of the data has <= 8 words\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.bar(np.arange(len(summary_length)),summary_length)\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('number of words')\n",
        "plt.xlim([0,20])\n",
        "plt.title('long review texts')\n",
        "print('For summaries, 90% of the data has <=', np.sum(summary_length_cdf<0.9), 'words,', '95% of the data has <=', np.sum(summary_length_cdf<0.95), 'words,', ' and 99% of the data has <=', np.sum(summary_length_cdf<0.99), 'words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuXoH78NZmeJ"
      },
      "source": [
        "## Pretrained embedding coverage\n",
        "In this project I used pretrained embeddings which usually greatly makes training faster. I compared two pretrained embeddings, glove50 and numberbatch. I checked how many words are covered by each embedding vocabulary and found glove50 is slightly better. Thus I used golve50 embeddings in this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EHuHna-TMLzS"
      },
      "outputs": [],
      "source": [
        "start_token, end_token = '<SOS>' , '<EOS>'\n",
        "summary['Summary'] = summary['Summary'].apply(lambda x: start_token + ' ' + x + ' ' + end_token)\n",
        "review_text['Text'] = review_text['Text'].apply(lambda x: start_token + ' ' + x + ' ' + end_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XKz2oeAPMLzS"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(list(summary['Summary'].values)+list(review_text['Text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nZ0UguePMLzT"
      },
      "outputs": [],
      "source": [
        "def read_embedding(filename):\n",
        "    \n",
        "    embeddings_index = {}\n",
        "    f = open(filename)\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    \n",
        "    return embeddings_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1mcoBlezMLzT"
      },
      "outputs": [],
      "source": [
        "embeddings_index1 = read_embedding('glove.6B/glove.6B.50d.txt')\n",
        "embeddings_index2 = read_embedding('numberbatch-en-19.08.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Cg9D90WQMLzT"
      },
      "outputs": [],
      "source": [
        "# Check how good the words in pre-trained embeddings cover the reviews\n",
        "# calculate total number of words and unique words in the review\n",
        "n_words = np.sum(list(tokenizer.word_counts.values()))\n",
        "i_words = len(tokenizer.word_counts)\n",
        "\n",
        "# calculate the number of words appearing larger than 3, 5, and 10 times in the review\n",
        "i_words_3 = sum(count>3 for (word,count) in tokenizer.word_counts.items())\n",
        "i_words_5 = sum(count>5 for (word,count) in tokenizer.word_counts.items())\n",
        "i_words_10 = sum(count>10 for (word,count) in tokenizer.word_counts.items())\n",
        "\n",
        "# calculate the percentage of the reviews consisting of words appear larger than 3, 5, and 10 times\n",
        "n_words_3 = sum(count*(count>3) for (word,count) in tokenizer.word_counts.items())\n",
        "n_words_5 = sum(count*(count>5) for (word,count) in tokenizer.word_counts.items())\n",
        "n_words_10 = sum(count*(count>10) for (word,count) in tokenizer.word_counts.items())\n",
        "\n",
        "# calculate the number of words appearing > 3,5,10 times in the reivew are covered by the pretrained embeddings\n",
        "i_words_3_1 = sum((count>3 and word in embeddings_index1) for (word,count) in tokenizer.word_counts.items())\n",
        "i_words_3_2 = sum((count>3 and word in embeddings_index2) for (word,count) in tokenizer.word_counts.items())\n",
        "i_words_5_1 = sum((count>5 and word in embeddings_index1) for (word,count) in tokenizer.word_counts.items())\n",
        "i_words_5_2 = sum((count>5 and word in embeddings_index2) for (word,count) in tokenizer.word_counts.items())\n",
        "i_words_10_1 = sum((count>10 and word in embeddings_index1) for (word,count) in tokenizer.word_counts.items())\n",
        "i_words_10_2 = sum((count>10 and word in embeddings_index2) for (word,count) in tokenizer.word_counts.items())\n",
        "\n",
        "# caculate the percentage of the words in reviews appearing > 3,5,10 times are covered by the pretrained embeddings\n",
        "n_words_3_1 = sum(count*(count>3 and word in embeddings_index1) for (word,count) in tokenizer.word_counts.items())\n",
        "n_words_3_2 = sum(count*(count>3 and word in embeddings_index2) for (word,count) in tokenizer.word_counts.items())\n",
        "n_words_5_1 = sum(count*(count>5 and word in embeddings_index1) for (word,count) in tokenizer.word_counts.items())\n",
        "n_words_5_2 = sum(count*(count>5 and word in embeddings_index2) for (word,count) in tokenizer.word_counts.items())\n",
        "n_words_10_1 = sum(count*(count>10 and word in embeddings_index1) for (word,count) in tokenizer.word_counts.items())\n",
        "n_words_10_2 = sum(count*(count>10 and word in embeddings_index2) for (word,count) in tokenizer.word_counts.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok4A3q-pMLzU",
        "outputId": "0bb65070-4c7b-4916-e09a-50bf449ee126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 116453 unique words in the reviews, the total word number is 27806392 \n",
            "\n",
            "42.0 % of the unique words appear > 3 times. 99.7 % of the review words are these words. For words appearing > 3 times, 76.0 % are covered by the pretrained embedding glove-50, 98.8 % of the review words are these words. 73.0 % are covered by the pretrained embedding numberbatch, 97.8 % of the review words are these words.\n",
            "\n",
            "34.0 % of the unique words appear > 5 times. 99.5 % of the review words are these words. For words appearing > 5 times, 81.0 % are covered by the pretrained embedding glove-50, 98.8 % of the review words are these words. 78.0 % are covered by the pretrained embedding numberbatch, 97.8 % of the review words are these words.\n",
            "\n",
            "25.0 % of the unique words appear > 10 times. 99.2 % of the review words are these words. For words appearing > 10 times, 87.0 % are covered by the pretrained embedding glove-50, 98.6 % of the review words are these words. 84.0 % are covered by the pretrained embedding numberbatch, 97.6 % of the review words are these words.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"There are\", i_words, \"unique words in the reviews, the total word number is\", n_words,'\\n')\n",
        "\n",
        "print(round(i_words_3/i_words,2)*100, '% of the unique words appear > 3 times.', round(n_words_3/n_words,3)*100, '% of the review words are these words. For words appearing > 3 times,', round(i_words_3_1/i_words_3,2)*100, '% are covered by the pretrained embedding glove-50,', round(n_words_3_1/n_words,3)*100, '% of the review words are these words.', round(i_words_3_2/i_words_3,2)*100, '% are covered by the pretrained embedding numberbatch,', round(n_words_3_2/n_words,3)*100, '% of the review words are these words.\\n')\n",
        "\n",
        "print(round(i_words_5/i_words,2)*100, '% of the unique words appear > 5 times.', round(n_words_5/n_words,3)*100, '% of the review words are these words. For words appearing > 5 times,', round(i_words_5_1/i_words_5,2)*100, '% are covered by the pretrained embedding glove-50,', round(n_words_5_1/n_words,3)*100, '% of the review words are these words.', round(i_words_5_2/i_words_5,2)*100, '% are covered by the pretrained embedding numberbatch,', round(n_words_5_2/n_words,3)*100, '% of the review words are these words.\\n')\n",
        "\n",
        "print(round(i_words_10/i_words,2)*100, '% of the unique words appear > 10 times.', round(n_words_10/n_words,3)*100, '% of the review words are these words. For words appearing > 10 times,', round(i_words_10_1/i_words_10,2)*100, '% are covered by the pretrained embedding glove-50,', round(n_words_10_1/n_words,3)*100, '% of the review words are these words.', round(i_words_10_2/i_words_10,2)*100, '% are covered by the pretrained embedding numberbatch,', round(n_words_10_2/n_words,3)*100, '% of the review words are these words.\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5YROJKbZmeO"
      },
      "source": [
        "### Missing words\n",
        "I checked what are missing from the glove50 embedding vocabulary. I found that the missing words are typically misspelling words or specific names. These words only appear < 10 times and account for only 2.4% of the reviews. So it's reasonable to neglect them. Thus I updated the vocabulary by excluding these words and used an \"< UNK >\" token to represent these words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJU_837JMLzV",
        "outputId": "2c2a884a-bfaf-44ef-9303-685f6c33ef4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jttodd', 'tastey', 'metromint', 'artisinal', 'yummmmmmm', 'snacky', 'superiour', 'kcup', 'sweetner', 'bustelo', 'zevia', 'coffe', 'larabar', 'nomnomnomnomnom', 'zukes', 'peanutbutter', 'krusteaz', 'mmmmh', 'jummy', 'yummies']\n"
          ]
        }
      ],
      "source": [
        "n = 20\n",
        "missingwords = []\n",
        "for (word,count) in tokenizer.word_counts.items():\n",
        "    if(n>0 and word not in embeddings_index1 and count > 10):\n",
        "        missingwords.append(word)\n",
        "        n -= 1\n",
        "print(missingwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PW3KC2wcMLzW"
      },
      "outputs": [],
      "source": [
        "x_tr, x_val, y_tr, y_val = train_test_split(np.array(review_text['Text']), np.array(summary['Summary']), test_size=0.1, random_state=0, shuffle=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wucTTOdfMLzW"
      },
      "outputs": [],
      "source": [
        "tokenizer_model = Tokenizer(num_words=i_words_10+1, oov_token='<UNK>')\n",
        "tokenizer_model.fit_on_texts(list(summary['Summary'].values)+list(review_text['Text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnQAJcsGMLzW",
        "outputId": "3bc3c571-635b-4b50-b7e7-e20b06146271"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 5048]]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "tokenizer_model.texts_to_sequences(['<EOS> awsome'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOR66NYpMLzX",
        "outputId": "c6168da7-e9dc-4f70-9670-a68981a865d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "new_dict = {}\n",
        "new_dict2 = {}\n",
        "new_dict['<UNK>'] = 1\n",
        "new_dict2[1] = '<UNK>'\n",
        "label = 2\n",
        "for (key, value) in tokenizer_model.word_index.items():\n",
        "    if(key != '<UNK>' and tokenizer_model.word_counts[key]>10 and key in embeddings_index1):\n",
        "        new_dict[key] = label\n",
        "        label +=1\n",
        "        new_dict2[value] = key\n",
        "\n",
        "tokenizer_model.word_index = new_dict\n",
        "tokenizer_model.index_word = new_dict2\n",
        "\n",
        "tokenizer_model.texts_to_sequences(['<EOS> choclate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiGij5r1ZmeT"
      },
      "source": [
        "## Preprocessing the data\n",
        "Now I have all information to decide how to preprocess the data. Using the token built in the last section I tokenized the words, and padded the texts by setting a maxium length of 100 words for review texts and 10 words for summaries, since ~90% of the examples have fewer words. The batch size was set to be 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "UIMfUG25MLzX"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(new_dict) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lAcvmV0DMLzY"
      },
      "outputs": [],
      "source": [
        "#convert text sequences into integer sequences\n",
        "x_tr_seq = tokenizer_model.texts_to_sequences(x_tr)\n",
        "y_tr_seq = tokenizer_model.texts_to_sequences(y_tr)\n",
        "x_val_seq = tokenizer_model.texts_to_sequences(x_val)\n",
        "y_val_seq = tokenizer_model.texts_to_sequences(y_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9arIUozNMLzZ"
      },
      "outputs": [],
      "source": [
        "max_len_text = 100\n",
        "max_len_summary = 10\n",
        "\n",
        "#padding zero up to maximum length\n",
        "x_tr_padded = pad_sequences(x_tr_seq,  maxlen=max_len_text, padding='post', truncating='post')\n",
        "x_val_padded = pad_sequences(x_val_seq, maxlen=max_len_text, padding='post', truncating='post')\n",
        "y_tr_padded = pad_sequences(y_tr_seq,  maxlen=max_len_summary, padding='post', truncating='post')\n",
        "y_val_padded = pad_sequences(y_val_seq, maxlen=max_len_summary, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "rZ4RPBk4MLzb"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((x_tr_padded, y_tr_padded)).batch(BATCH_SIZE)\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((x_val_padded,y_val_padded)).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZPnlam2ZmeX"
      },
      "source": [
        "## Model\n",
        "Here I used subclassing API to write the model. This way enables changing hyperparameters such as number of heads, number of layers and dropout rate globally. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_JCppC-PVHmY"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_embeddings(filename, embedding_dim=50, vocab_size=100000):\n",
        "    \n",
        "    if(filename=='glove.6B/glove.6B.50d.txt'):\n",
        "        embedding_dim = 50\n",
        "        vocab_size = VOCAB_SIZE\n",
        "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "        for word, i in tokenizer_model.word_index.items():\n",
        "            embedding_vector = embeddings_index1.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "        embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
        "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                            trainable=False)\n",
        "    \n",
        "    return embedding_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "U_R-nbwgSifL"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_rl0nd4lSkCZ"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ymvWLq_SSnZA"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Phtr58hrSnf5"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "wItY-D5_Snn0"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "HOQtUqeqSnux"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mLVjKegjSn5f"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-qsGNG64Sn_5"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "df0GRMK7TOsC"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "-OILurTDTOyN"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1, embedding_filename=None):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    if(embedding_filename):\n",
        "      self.embedding = load_pretrained_embeddings(filename=embedding_filename)\n",
        "    else:\n",
        "      self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "rp2Y1ZOKTO8i"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1, embedding_filename=None):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    if(embedding_filename):\n",
        "      self.embedding = load_pretrained_embeddings(filename=embedding_filename)\n",
        "    else:\n",
        "      self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7q80zbnITPC1"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1, embedding_filename=None):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate,\n",
        "                           embedding_filename=embedding_filename)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                           target_vocab_size, pe_target, rate,\n",
        "                           embedding_filename=embedding_filename)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    # Keras models prefer if you pass all your inputs in the first argument\n",
        "    inp, tar = inputs\n",
        "\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, look_ahead_mask, dec_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pExv7dQsZmed"
      },
      "source": [
        "### Construct the model\n",
        "The hyperparameters of the model are shown below. Emprically I found a deep model (large num_layers) is very helpful for generating meaningful summerizations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "UBchRBqTMLzl"
      },
      "outputs": [],
      "source": [
        "transformer_model = Transformer(num_layers=5,\n",
        "                                d_model=50,\n",
        "                                num_heads=5,\n",
        "                                dff=512,\n",
        "                                input_vocab_size=VOCAB_SIZE,\n",
        "                                target_vocab_size=VOCAB_SIZE,\n",
        "                                pe_input=max_len_text,\n",
        "                                pe_target=max_len_text,\n",
        "                                rate=0.15,\n",
        "                                embedding_filename='glove.6B/glove.6B.50d.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ7Z1b5dMLzl"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wadRAJxrMLzl"
      },
      "outputs": [],
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-4, decay_steps=4000, decay_rate=0.95)\n",
        "optimizer = Adam(lr_schedule , beta_1=0.9, beta_2=0.98, epsilon=1e-9) \n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "#train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "XJ82LDfqMub5"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer_model, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "QTHK8VXoMLzl"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    \n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fN0EhfEoJTPV"
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    #print('validation started ...')\n",
        "    val_loss.reset_states()\n",
        "    for (batch, (inp, tar)) in enumerate(dataset_val):    \n",
        "        tar_inp = tar[:, :-1] # <startseq> hi im moein\n",
        "        tar_real = tar[:, 1:] # hi im moein <endseq>\n",
        "        predictions, _ = transformer_model(inputs=[inp, tar_inp], training=False)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        val_loss(loss)\n",
        "    #print('\\n* Validation loss: {} '.format(val_loss.result()) )\n",
        "    return val_loss.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "umKVE2-1MLzm"
      },
      "outputs": [],
      "source": [
        "@tf.function # Compiles a function into a callable TensorFlow graph\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1] \n",
        "    tar_real = tar[:, 1:] \n",
        "\n",
        "    #enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer_model(inputs=[inp, tar_inp], training=True)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer_model.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer_model.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZr2BQVpMLzm",
        "outputId": "f39f8849-41e6-4242-818f-cfcb5858a8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 10.1490 Time 10.37 secs\n",
            "Epoch 1 Batch 2000 Loss 6.4812 Time 80.41 secs\n",
            "Epoch 1 Batch 4000 Loss 5.9650 Time 150.40 secs\n",
            "Epoch 1 Batch 6000 Loss 5.7492 Time 220.40 secs\n",
            "Epoch 1 Batch 7954 Train Loss 5.6151 Valication Loss 5.1158 Time 496.09 secs\n",
            "Epoch 2 Batch 0 Loss 5.3514 Time 0.16 secs\n",
            "Epoch 2 Batch 2000 Loss 5.1323 Time 70.14 secs\n",
            "Epoch 2 Batch 4000 Loss 5.1004 Time 140.11 secs\n",
            "Epoch 2 Batch 6000 Loss 5.0719 Time 210.10 secs\n",
            "Epoch 2 Batch 7954 Train Loss 5.0465 Valication Loss 4.9018 Time 431.71 secs\n",
            "Epoch 3 Batch 0 Loss 5.0905 Time 0.10 secs\n",
            "Epoch 3 Batch 2000 Loss 4.9430 Time 70.03 secs\n",
            "Epoch 3 Batch 4000 Loss 4.9244 Time 139.95 secs\n",
            "Epoch 3 Batch 6000 Loss 4.9060 Time 209.88 secs\n",
            "Epoch 3 Batch 7954 Train Loss 4.8891 Valication Loss 4.7704 Time 429.85 secs\n",
            "Epoch 4 Batch 0 Loss 4.9592 Time 0.10 secs\n",
            "Epoch 4 Batch 2000 Loss 4.8254 Time 70.10 secs\n",
            "Epoch 4 Batch 4000 Loss 4.8116 Time 140.12 secs\n",
            "Epoch 4 Batch 6000 Loss 4.7974 Time 210.13 secs\n",
            "Epoch 4 Batch 7954 Train Loss 4.7845 Valication Loss 4.6810 Time 429.70 secs\n",
            "Epoch 5 Batch 0 Loss 4.8725 Time 0.10 secs\n",
            "Epoch 5 Batch 2000 Loss 4.7406 Time 70.11 secs\n",
            "Epoch 5 Batch 4000 Loss 4.7292 Time 140.13 secs\n",
            "Epoch 5 Batch 6000 Loss 4.7174 Time 210.14 secs\n",
            "Epoch 5 Batch 7954 Train Loss 4.7061 Valication Loss 4.6076 Time 429.06 secs\n",
            "Epoch 6 Batch 0 Loss 4.7930 Time 0.10 secs\n",
            "Epoch 6 Batch 2000 Loss 4.6710 Time 70.03 secs\n",
            "Epoch 6 Batch 4000 Loss 4.6627 Time 140.00 secs\n",
            "Epoch 6 Batch 6000 Loss 4.6526 Time 209.98 secs\n",
            "Epoch 6 Batch 7954 Train Loss 4.6430 Valication Loss 4.5538 Time 428.08 secs\n",
            "Epoch 7 Batch 0 Loss 4.7283 Time 0.10 secs\n",
            "Epoch 7 Batch 2000 Loss 4.6157 Time 70.10 secs\n",
            "Epoch 7 Batch 4000 Loss 4.6090 Time 140.11 secs\n",
            "Epoch 7 Batch 6000 Loss 4.6008 Time 210.11 secs\n",
            "Epoch 7 Batch 7954 Train Loss 4.5924 Valication Loss 4.5144 Time 428.76 secs\n",
            "Epoch 8 Batch 0 Loss 4.6589 Time 0.11 secs\n",
            "Epoch 8 Batch 2000 Loss 4.5731 Time 70.08 secs\n",
            "Epoch 8 Batch 4000 Loss 4.5677 Time 140.05 secs\n",
            "Epoch 8 Batch 6000 Loss 4.5610 Time 210.02 secs\n",
            "Epoch 8 Batch 7954 Train Loss 4.5540 Valication Loss 4.4828 Time 428.35 secs\n",
            "Epoch 9 Batch 0 Loss 4.6190 Time 0.10 secs\n",
            "Epoch 9 Batch 2000 Loss 4.5390 Time 70.08 secs\n",
            "Epoch 9 Batch 4000 Loss 4.5355 Time 140.02 secs\n",
            "Epoch 9 Batch 6000 Loss 4.5296 Time 209.95 secs\n",
            "Epoch 9 Batch 7954 Train Loss 4.5230 Valication Loss 4.4581 Time 427.84 secs\n",
            "Epoch 10 Batch 0 Loss 4.6083 Time 0.10 secs\n",
            "Epoch 10 Batch 2000 Loss 4.5111 Time 70.14 secs\n",
            "Epoch 10 Batch 4000 Loss 4.5080 Time 140.05 secs\n",
            "Epoch 10 Batch 6000 Loss 4.5028 Time 209.97 secs\n",
            "Epoch 10 Batch 7954 Train Loss 4.4973 Valication Loss 4.4400 Time 428.49 secs\n",
            "Epoch 11 Batch 0 Loss 4.5634 Time 0.10 secs\n",
            "Epoch 11 Batch 2000 Loss 4.4889 Time 70.13 secs\n",
            "Epoch 11 Batch 4000 Loss 4.4868 Time 140.14 secs\n",
            "Epoch 11 Batch 6000 Loss 4.4819 Time 210.15 secs\n",
            "Epoch 11 Batch 7954 Train Loss 4.4766 Valication Loss 4.4219 Time 429.13 secs\n",
            "Epoch 12 Batch 0 Loss 4.5054 Time 0.10 secs\n",
            "Epoch 12 Batch 2000 Loss 4.4709 Time 70.05 secs\n",
            "Epoch 12 Batch 4000 Loss 4.4696 Time 140.02 secs\n",
            "Epoch 12 Batch 6000 Loss 4.4648 Time 210.11 secs\n",
            "Epoch 12 Batch 7954 Train Loss 4.4597 Valication Loss 4.4061 Time 428.61 secs\n",
            "Epoch 13 Batch 0 Loss 4.5470 Time 0.10 secs\n",
            "Epoch 13 Batch 2000 Loss 4.4558 Time 70.11 secs\n",
            "Epoch 13 Batch 4000 Loss 4.4548 Time 140.11 secs\n",
            "Epoch 13 Batch 6000 Loss 4.4499 Time 210.10 secs\n",
            "Epoch 13 Batch 7954 Train Loss 4.4447 Valication Loss 4.3958 Time 428.99 secs\n",
            "Epoch 14 Batch 0 Loss 4.4793 Time 0.10 secs\n",
            "Epoch 14 Batch 2000 Loss 4.4448 Time 70.10 secs\n",
            "Epoch 14 Batch 4000 Loss 4.4427 Time 140.10 secs\n",
            "Epoch 14 Batch 6000 Loss 4.4381 Time 210.13 secs\n",
            "Epoch 14 Batch 7954 Train Loss 4.4333 Valication Loss 4.3870 Time 429.27 secs\n",
            "Epoch 15 Batch 0 Loss 4.5049 Time 0.10 secs\n",
            "Epoch 15 Batch 2000 Loss 4.4330 Time 70.12 secs\n",
            "Epoch 15 Batch 4000 Loss 4.4317 Time 140.13 secs\n",
            "Epoch 15 Batch 6000 Loss 4.4279 Time 210.14 secs\n",
            "Epoch 15 Batch 7954 Train Loss 4.4234 Valication Loss 4.3775 Time 428.89 secs\n",
            "Epoch 16 Batch 0 Loss 4.4623 Time 0.10 secs\n",
            "Epoch 16 Batch 2000 Loss 4.4240 Time 70.09 secs\n",
            "Epoch 16 Batch 4000 Loss 4.4229 Time 140.08 secs\n",
            "Epoch 16 Batch 6000 Loss 4.4188 Time 210.08 secs\n",
            "Epoch 16 Batch 7954 Train Loss 4.4146 Valication Loss 4.3694 Time 429.18 secs\n",
            "Epoch 17 Batch 0 Loss 4.5231 Time 0.10 secs\n",
            "Epoch 17 Batch 2000 Loss 4.4171 Time 70.10 secs\n",
            "Epoch 17 Batch 4000 Loss 4.4159 Time 140.09 secs\n",
            "Epoch 17 Batch 6000 Loss 4.4117 Time 210.12 secs\n",
            "Epoch 17 Batch 7954 Train Loss 4.4070 Valication Loss 4.3630 Time 428.64 secs\n",
            "Epoch 18 Batch 0 Loss 4.4648 Time 0.10 secs\n",
            "Epoch 18 Batch 2000 Loss 4.4095 Time 70.10 secs\n",
            "Epoch 18 Batch 4000 Loss 4.4089 Time 140.10 secs\n",
            "Epoch 18 Batch 6000 Loss 4.4052 Time 210.11 secs\n",
            "Epoch 18 Batch 7954 Train Loss 4.4010 Valication Loss 4.3563 Time 428.72 secs\n",
            "Epoch 19 Batch 0 Loss 4.4448 Time 0.10 secs\n",
            "Epoch 19 Batch 2000 Loss 4.4025 Time 70.08 secs\n",
            "Epoch 19 Batch 4000 Loss 4.4019 Time 140.09 secs\n",
            "Epoch 19 Batch 6000 Loss 4.3983 Time 210.10 secs\n",
            "Epoch 19 Batch 7954 Train Loss 4.3946 Valication Loss 4.3502 Time 428.77 secs\n",
            "Epoch 20 Batch 0 Loss 4.4557 Time 0.10 secs\n",
            "Epoch 20 Batch 2000 Loss 4.3985 Time 70.10 secs\n",
            "Epoch 20 Batch 4000 Loss 4.3976 Time 140.12 secs\n",
            "Epoch 20 Batch 6000 Loss 4.3941 Time 210.10 secs\n",
            "Epoch 20 Batch 7954 Train Loss 4.3904 Valication Loss 4.3459 Time 429.34 secs\n",
            "Epoch 21 Batch 0 Loss 4.4469 Time 0.10 secs\n",
            "Epoch 21 Batch 2000 Loss 4.3942 Time 70.12 secs\n",
            "Epoch 21 Batch 4000 Loss 4.3940 Time 140.14 secs\n",
            "Epoch 21 Batch 6000 Loss 4.3906 Time 210.17 secs\n",
            "Epoch 21 Batch 7954 Train Loss 4.3863 Valication Loss 4.3408 Time 428.63 secs\n",
            "Epoch 22 Batch 0 Loss 4.4160 Time 0.10 secs\n",
            "Epoch 22 Batch 2000 Loss 4.3898 Time 70.11 secs\n",
            "Epoch 22 Batch 4000 Loss 4.3907 Time 140.11 secs\n",
            "Epoch 22 Batch 6000 Loss 4.3870 Time 210.10 secs\n",
            "Epoch 22 Batch 7954 Train Loss 4.3833 Valication Loss 4.3387 Time 429.06 secs\n",
            "Epoch 23 Batch 0 Loss 4.4117 Time 0.10 secs\n",
            "Epoch 23 Batch 2000 Loss 4.3883 Time 70.09 secs\n",
            "Epoch 23 Batch 4000 Loss 4.3883 Time 140.10 secs\n",
            "Epoch 23 Batch 6000 Loss 4.3846 Time 210.10 secs\n",
            "Epoch 23 Batch 7954 Train Loss 4.3810 Valication Loss 4.3343 Time 429.32 secs\n",
            "Epoch 24 Batch 0 Loss 4.4230 Time 0.10 secs\n",
            "Epoch 24 Batch 2000 Loss 4.3844 Time 70.06 secs\n",
            "Epoch 24 Batch 4000 Loss 4.3849 Time 140.02 secs\n",
            "Epoch 24 Batch 6000 Loss 4.3819 Time 209.96 secs\n",
            "Epoch 24 Batch 7954 Train Loss 4.3782 Valication Loss 4.3310 Time 428.89 secs\n",
            "Epoch 25 Batch 0 Loss 4.4457 Time 0.10 secs\n",
            "Epoch 25 Batch 2000 Loss 4.3814 Time 70.05 secs\n",
            "Epoch 25 Batch 4000 Loss 4.3826 Time 140.04 secs\n",
            "Epoch 25 Batch 6000 Loss 4.3795 Time 210.01 secs\n",
            "Epoch 25 Batch 7954 Train Loss 4.3760 Valication Loss 4.3294 Time 428.75 secs\n",
            "Epoch 26 Batch 0 Loss 4.3957 Time 0.10 secs\n",
            "Epoch 26 Batch 2000 Loss 4.3800 Time 70.03 secs\n",
            "Epoch 26 Batch 4000 Loss 4.3812 Time 139.96 secs\n",
            "Epoch 26 Batch 6000 Loss 4.3781 Time 209.91 secs\n",
            "Epoch 26 Batch 7954 Train Loss 4.3743 Valication Loss 4.3256 Time 429.04 secs\n",
            "Epoch 27 Batch 0 Loss 4.3828 Time 0.10 secs\n",
            "Epoch 27 Batch 2000 Loss 4.3780 Time 70.10 secs\n",
            "Epoch 27 Batch 4000 Loss 4.3785 Time 140.09 secs\n",
            "Epoch 27 Batch 6000 Loss 4.3757 Time 210.09 secs\n",
            "Epoch 27 Batch 7954 Train Loss 4.3722 Valication Loss 4.3225 Time 480.79 secs\n",
            "Epoch 28 Batch 0 Loss 4.4786 Time 0.10 secs\n",
            "Epoch 28 Batch 2000 Loss 4.3767 Time 70.12 secs\n",
            "Epoch 28 Batch 4000 Loss 4.3777 Time 140.11 secs\n",
            "Epoch 28 Batch 6000 Loss 4.3749 Time 210.10 secs\n",
            "Epoch 28 Batch 7954 Train Loss 4.3716 Valication Loss 4.3193 Time 428.93 secs\n",
            "Epoch 29 Batch 0 Loss 4.4071 Time 0.10 secs\n",
            "Epoch 29 Batch 2000 Loss 4.3761 Time 70.10 secs\n",
            "Epoch 29 Batch 4000 Loss 4.3774 Time 140.08 secs\n",
            "Epoch 29 Batch 6000 Loss 4.3737 Time 210.08 secs\n",
            "Epoch 29 Batch 7954 Train Loss 4.3703 Valication Loss 4.3175 Time 429.28 secs\n",
            "Epoch 30 Batch 0 Loss 4.4395 Time 0.10 secs\n",
            "Epoch 30 Batch 2000 Loss 4.3750 Time 70.09 secs\n",
            "Epoch 30 Batch 4000 Loss 4.3763 Time 140.11 secs\n",
            "Epoch 30 Batch 6000 Loss 4.3731 Time 210.11 secs\n",
            "Epoch 30 Batch 7954 Train Loss 4.3695 Valication Loss 4.3156 Time 429.16 secs\n",
            "Epoch 31 Batch 0 Loss 4.3892 Time 0.10 secs\n",
            "Epoch 31 Batch 2000 Loss 4.3745 Time 70.09 secs\n",
            "Epoch 31 Batch 4000 Loss 4.3753 Time 140.06 secs\n",
            "Epoch 31 Batch 6000 Loss 4.3718 Time 210.05 secs\n",
            "Epoch 31 Batch 7954 Train Loss 4.3686 Valication Loss 4.3139 Time 428.97 secs\n",
            "Epoch 32 Batch 0 Loss 4.4330 Time 0.10 secs\n",
            "Epoch 32 Batch 2000 Loss 4.3740 Time 70.09 secs\n",
            "Epoch 32 Batch 4000 Loss 4.3747 Time 140.09 secs\n",
            "Epoch 32 Batch 6000 Loss 4.3717 Time 210.10 secs\n",
            "Epoch 32 Batch 7954 Train Loss 4.3681 Valication Loss 4.3121 Time 428.94 secs\n",
            "Epoch 33 Batch 0 Loss 4.3851 Time 0.10 secs\n",
            "Epoch 33 Batch 2000 Loss 4.3736 Time 70.11 secs\n",
            "Epoch 33 Batch 4000 Loss 4.3745 Time 140.11 secs\n",
            "Epoch 33 Batch 6000 Loss 4.3715 Time 210.10 secs\n",
            "Epoch 33 Batch 7954 Train Loss 4.3682 Valication Loss 4.3112 Time 428.55 secs\n",
            "Epoch 34 Batch 0 Loss 4.4125 Time 0.10 secs\n",
            "Epoch 34 Batch 2000 Loss 4.3726 Time 70.11 secs\n",
            "Epoch 34 Batch 4000 Loss 4.3740 Time 140.11 secs\n",
            "Epoch 34 Batch 6000 Loss 4.3708 Time 210.12 secs\n",
            "Epoch 34 Batch 7954 Train Loss 4.3673 Valication Loss 4.3096 Time 429.10 secs\n",
            "Epoch 35 Batch 0 Loss 4.3988 Time 0.10 secs\n",
            "Epoch 35 Batch 2000 Loss 4.3712 Time 70.02 secs\n",
            "Epoch 35 Batch 4000 Loss 4.3729 Time 139.97 secs\n",
            "Epoch 35 Batch 6000 Loss 4.3700 Time 209.89 secs\n",
            "Epoch 35 Batch 7954 Train Loss 4.3666 Valication Loss 4.3087 Time 480.53 secs\n",
            "Epoch 36 Batch 0 Loss 4.3915 Time 0.10 secs\n",
            "Epoch 36 Batch 2000 Loss 4.3722 Time 70.03 secs\n",
            "Epoch 36 Batch 4000 Loss 4.3737 Time 139.98 secs\n",
            "Epoch 36 Batch 6000 Loss 4.3707 Time 209.93 secs\n",
            "Epoch 36 Batch 7954 Train Loss 4.3672 Valication Loss 4.3068 Time 429.52 secs\n",
            "Epoch 37 Batch 0 Loss 4.4364 Time 0.10 secs\n",
            "Epoch 37 Batch 2000 Loss 4.3713 Time 70.10 secs\n",
            "Epoch 37 Batch 4000 Loss 4.3733 Time 140.08 secs\n",
            "Epoch 37 Batch 6000 Loss 4.3708 Time 210.09 secs\n",
            "Epoch 37 Batch 7954 Train Loss 4.3673 Valication Loss 4.3060 Time 428.94 secs\n",
            "Epoch 38 Batch 0 Loss 4.4052 Time 0.10 secs\n",
            "Epoch 38 Batch 2000 Loss 4.3733 Time 70.14 secs\n",
            "Epoch 38 Batch 4000 Loss 4.3739 Time 140.14 secs\n",
            "Epoch 38 Batch 6000 Loss 4.3711 Time 210.13 secs\n",
            "Epoch 38 Batch 7954 Train Loss 4.3678 Valication Loss 4.3058 Time 429.33 secs\n",
            "Epoch 39 Batch 0 Loss 4.4020 Time 0.10 secs\n",
            "Epoch 39 Batch 2000 Loss 4.3724 Time 70.09 secs\n",
            "Epoch 39 Batch 4000 Loss 4.3737 Time 140.08 secs\n",
            "Epoch 39 Batch 6000 Loss 4.3704 Time 210.08 secs\n",
            "Epoch 39 Batch 7954 Train Loss 4.3673 Valication Loss 4.3043 Time 428.73 secs\n",
            "Epoch 40 Batch 0 Loss 4.3993 Time 0.10 secs\n",
            "Epoch 40 Batch 2000 Loss 4.3720 Time 70.09 secs\n",
            "Epoch 40 Batch 4000 Loss 4.3735 Time 140.09 secs\n",
            "Epoch 40 Batch 6000 Loss 4.3704 Time 210.11 secs\n",
            "Epoch 40 Batch 7954 Train Loss 4.3668 Valication Loss 4.3039 Time 430.13 secs\n",
            "Epoch 41 Batch 0 Loss 4.4447 Time 0.10 secs\n",
            "Epoch 41 Batch 2000 Loss 4.3724 Time 70.09 secs\n",
            "Epoch 41 Batch 4000 Loss 4.3730 Time 140.08 secs\n",
            "Epoch 41 Batch 6000 Loss 4.3703 Time 210.14 secs\n",
            "Epoch 41 Batch 7954 Train Loss 4.3668 Valication Loss 4.3030 Time 429.60 secs\n",
            "Epoch 42 Batch 0 Loss 4.4336 Time 0.10 secs\n",
            "Epoch 42 Batch 2000 Loss 4.3715 Time 70.11 secs\n",
            "Epoch 42 Batch 4000 Loss 4.3733 Time 140.11 secs\n",
            "Epoch 42 Batch 6000 Loss 4.3709 Time 210.11 secs\n",
            "Epoch 42 Batch 7954 Train Loss 4.3675 Valication Loss 4.3027 Time 429.55 secs\n",
            "Epoch 43 Batch 0 Loss 4.4280 Time 0.10 secs\n",
            "Epoch 43 Batch 2000 Loss 4.3721 Time 70.10 secs\n",
            "Epoch 43 Batch 4000 Loss 4.3732 Time 140.09 secs\n",
            "Epoch 43 Batch 6000 Loss 4.3706 Time 210.07 secs\n",
            "Epoch 43 Batch 7954 Train Loss 4.3673 Valication Loss 4.3017 Time 430.40 secs\n",
            "Epoch 44 Batch 0 Loss 4.3855 Time 0.10 secs\n",
            "Epoch 44 Batch 2000 Loss 4.3731 Time 70.09 secs\n",
            "Epoch 44 Batch 4000 Loss 4.3735 Time 140.08 secs\n",
            "Epoch 44 Batch 6000 Loss 4.3709 Time 210.09 secs\n",
            "Epoch 44 Batch 7954 Train Loss 4.3672 Valication Loss 4.3013 Time 429.98 secs\n",
            "Epoch 45 Batch 0 Loss 4.3620 Time 0.10 secs\n",
            "Epoch 45 Batch 2000 Loss 4.3726 Time 70.12 secs\n",
            "Epoch 45 Batch 4000 Loss 4.3733 Time 140.13 secs\n",
            "Epoch 45 Batch 6000 Loss 4.3701 Time 210.14 secs\n",
            "Epoch 45 Batch 7954 Train Loss 4.3671 Valication Loss 4.3008 Time 430.18 secs\n",
            "Epoch 46 Batch 0 Loss 4.3825 Time 0.10 secs\n",
            "Epoch 46 Batch 2000 Loss 4.3725 Time 70.18 secs\n",
            "Epoch 46 Batch 4000 Loss 4.3737 Time 140.29 secs\n",
            "Epoch 46 Batch 6000 Loss 4.3706 Time 210.29 secs\n",
            "Epoch 46 Batch 7954 Train Loss 4.3672 Valication Loss 4.3006 Time 429.33 secs\n",
            "Epoch 47 Batch 0 Loss 4.3822 Time 0.10 secs\n",
            "Epoch 47 Batch 2000 Loss 4.3733 Time 70.10 secs\n",
            "Epoch 47 Batch 4000 Loss 4.3734 Time 140.08 secs\n",
            "Epoch 47 Batch 6000 Loss 4.3709 Time 210.09 secs\n",
            "Epoch 47 Batch 7954 Train Loss 4.3675 Valication Loss 4.3003 Time 429.21 secs\n",
            "Epoch 48 Batch 0 Loss 4.4134 Time 0.10 secs\n",
            "Epoch 48 Batch 2000 Loss 4.3727 Time 70.04 secs\n",
            "Epoch 48 Batch 4000 Loss 4.3733 Time 140.01 secs\n",
            "Epoch 48 Batch 6000 Loss 4.3708 Time 209.94 secs\n",
            "Epoch 48 Batch 7954 Train Loss 4.3673 Valication Loss 4.2998 Time 429.84 secs\n",
            "Epoch 49 Batch 0 Loss 4.3823 Time 0.10 secs\n",
            "Epoch 49 Batch 2000 Loss 4.3728 Time 70.09 secs\n",
            "Epoch 49 Batch 4000 Loss 4.3734 Time 140.07 secs\n",
            "Epoch 49 Batch 6000 Loss 4.3709 Time 210.08 secs\n",
            "Epoch 49 Batch 7954 Train Loss 4.3676 Valication Loss 4.2997 Time 429.75 secs\n",
            "Epoch 50 Batch 0 Loss 4.4429 Time 0.10 secs\n",
            "Epoch 50 Batch 2000 Loss 4.3713 Time 70.09 secs\n",
            "Epoch 50 Batch 4000 Loss 4.3726 Time 140.08 secs\n",
            "Epoch 50 Batch 6000 Loss 4.3705 Time 210.07 secs\n",
            "Epoch 50 Batch 7954 Train Loss 4.3673 Valication Loss 4.2993 Time 430.08 secs\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "history = {'train_loss':[], 'val_loss':[]}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    #train_accuracy.reset_states()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset_train):\n",
        "        train_step(inp, tar)\n",
        "        if batch % 2000 == 0:\n",
        "            #validate_loss = validate().numpy()\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Time {time.time() - start:.2f} secs')\n",
        "            #print(f'Epoch {epoch + 1} Batch {batch} Train Loss {train_loss.result():.4f} Valication Loss {validate_loss:.4f} Time {time.time() - start:.2f} secs')\n",
        "    trained_loss = train_loss.result().numpy()\n",
        "    validate_loss = validate().numpy()\n",
        "    history['train_loss'].append(trained_loss)\n",
        "    history['val_loss'].append(validate_loss)\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Epoch {epoch + 1} Batch {batch} Train Loss {train_loss.result():.4f} Valication Loss {validate_loss:.4f} Time {time.time() - start:.2f} secs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ENIOtu1ILIxK",
        "outputId": "998140fe-62a0-4b93-cf1a-de3ff62c8fb4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.plot(np.arange(EPOCHS), history['train_loss'], label = 'train')\n",
        "plt.plot(np.arange(EPOCHS), history['val_loss'], label = 'evaluation')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-apPC_qEANlO",
        "outputId": "94aa0150-40c9-4a63-8220-c4e01a389b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  1603360   \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  1654860   \n",
            "                                                                 \n",
            " dense_80 (Dense)            multiple                  1318401   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,576,621\n",
            "Trainable params: 1,991,521\n",
            "Non-trainable params: 2,585,100\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY4AiuQbZmei"
      },
      "source": [
        "## Generate summerization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "QUL3Gu5VMLzm"
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    print('validation started ...')\n",
        "    val_loss.reset_states()\n",
        "    for (batch, (inp, tar)) in enumerate(dataset_val):    \n",
        "        tar_inp = tar[:, :-1] \n",
        "        tar_real = tar[:, 1:] \n",
        "        predictions, _ = transformer_model(inputs=[inp, tar_inp], training=False)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        val_loss(loss)\n",
        "    print('\\n* Validation loss: {} '.format(val_loss.result()) )\n",
        "    \n",
        "    return val_loss.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Io3Y6paqMLzn"
      },
      "outputs": [],
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = tokenizer_model.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=max_len_text, \n",
        "                                                                           padding='post', truncating='post')\n",
        "    \n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "    start_token = 'sos'\n",
        "    decoder_input = [tokenizer_model.word_index[start_token]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_len_summary):\n",
        "        \n",
        "        predictions, attention_weights = transformer_model(inputs=[encoder_input, output], training = False)\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        # stop prediciting if it reached end_token\n",
        "        end_token = 'eos'\n",
        "        if predicted_id == tokenizer_model.word_index[end_token]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # remove start_token\n",
        "    return tokenizer_model.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkUcrGArAJKu",
        "outputId": "acf592ff-7fae-43c7-9a84-5a6d895ba6b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original review: <SOS> love love love tea amazing actually bought gift someone year ended getting well shipping great time holidays bad comes around year would drink year round would even better loose leaf see option pod option <EOS>\n",
            "generated summary: great tea\n",
            "original summary: <SOS> best gift give receive <EOS>\n"
          ]
        }
      ],
      "source": [
        "i1 = 359\n",
        "print('original review:', x_val[i1])\n",
        "print('generated summary:',summarize(clean_data(x_val[i1])))\n",
        "print('original summary:', y_val[i1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xV_QFmuZmej",
        "outputId": "0888610a-e744-493d-af66-10701a4bb619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original review: <SOS> okay told amazon review way short needed post review actual product made comment shipping charges way expensive whatever needs said <br ><br >anyway chorizo chorizo looking small links grew used much dishes many products buy even come close real thing real thing alot paprika still 2 links left saving rainy day despite shipping charges would buy <EOS>\n",
            "generated summary: good product\n",
            "original summary: <SOS> delicioso <EOS>\n"
          ]
        }
      ],
      "source": [
        "i2 = 125\n",
        "print('original review:', x_val[i2])\n",
        "print('generated summary:',summarize(clean_data(x_val[i2])))\n",
        "print('original summary:', y_val[i2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i3 = 4857\n",
        "print('original review:', x_val[i3])\n",
        "print('generated summary:',summarize(clean_data(x_val[i3])))\n",
        "print('original summary:', y_val[i3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVVYYkcoZwCc",
        "outputId": "75aa4046-f3de-49bc-d63d-fe6972cf7f35"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original review: <SOS> yummy minty really minty really yummy <br ><br >the plastic packaging bit large pants shirt pocket handy counter desk drawer ir coat pocket 1 2 top case flips open lid works well <EOS>\n",
            "generated summary: great product\n",
            "original summary: <SOS> good strong mints <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ry5F250kaP73"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text_summerizer_transformer_model_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}